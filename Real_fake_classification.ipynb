{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('fake_or_real_news_training.csv')\n",
    "test=pd.read_csv('fake_or_real_news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import subprocess \n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "###########################################\n",
    "##### Define text cleaning function #######\n",
    "###########################################\n",
    "def text_cleaning(text, escape_list=[], stop=[]):\n",
    "    l=[]\n",
    "    \"\"\"\n",
    "    Text cleaning function:\n",
    "        Input: \n",
    "            -text: a string variable, the text to be cleaned\n",
    "            -escape_list : words not to transform by the cleaning process (only lowcase transformation is needed)  \n",
    "            -stop : custom stopwords\n",
    "        Output:\n",
    "            -text cleaned and stemmed           \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" Get stop word list from package\"\"\"\n",
    "    #STOPWORDS ARE COMMENTED\n",
    "    #StopWords = list(set(stopwords.words('english')))\n",
    "    StopWords=[]\n",
    "    custom_stop = StopWords + stop\n",
    "    \n",
    "    \"\"\" Step 1: Parse html entities\"\"\"\n",
    "    text = html.unescape(text)\n",
    "    text=text.replace('\\n',' ').replace('\\t',' ').replace('â€™','')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" Step 2: Decode special caracters\"\"\"\n",
    "    text = text.encode('utf8').decode('unicode_escape')\n",
    "    \n",
    " \n",
    "    \"\"\" Step 3: Tokenise text: spliting text elements with the TreeBankWordTokenizer method\"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokenz=[','.join(tokenizer.tokenize(mot)) if mot  not in escape_list else mot  for mot in text.split()  ]\n",
    "    \n",
    "    \n",
    "    \"\"\" Step 4: Drop punctuations \"\"\"\n",
    "    tokenz=[re.sub(r'[^\\w\\s]',' ',mot) if mot  not in escape_list else mot  for mot in tokenz  ]\n",
    "    tokenz = ' '.join(tokenz).split()\n",
    "       \n",
    "    \"\"\" Step 5.1: Remove stop words \"\"\"\n",
    "    tokenz=([token for token in tokenz if token not in custom_stop])\n",
    "    \n",
    "    \n",
    "    \"\"\" Step 5.2: Delete digits from text \"\"\"\n",
    "    #tokenz=([token for token in tokenz if (  (token.isdigit())==False)  ])  \n",
    "\n",
    "    \"\"\" Step 5.3: Remove digits from tokens\"\"\"\n",
    "    #remove_digits = str.maketrans('', '', digits)\n",
    "    #tokenz=[token.translate(remove_digits)  if token not in  escape_list else token for token in tokenz   ]\n",
    "    \n",
    "    \"\"\" Step 6.1: Lowcase the text\"\"\"\n",
    "    tokenz=([token.lower() for token in tokenz])\n",
    "    \n",
    "    \"\"\" Step 6.2: Lemmatize the text \n",
    "     \n",
    "'''tokenz=[WordNetLemmatizer().lemmatize(token) if token not in escape_list else token for token in tokenz ]'''\"\"\"\n",
    "    \"\"\" Step 6.2: Stem the text \"\"\"\n",
    "    tokenz=[EnglishStemmer().stem(token) if token not in escape_list else token for token in tokenz ]\n",
    "\n",
    "    \"\"\" Step 6.3: Drop words with one caratcter and proceed last check for stop words after Stemming\"\"\"\n",
    "    tokenz=[token for token in tokenz if (token not in  custom_stop and len(token)>1) ]\n",
    "\n",
    "    return ' '.join(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'title', 'text', 'label', 'X1', 'X2'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abandyop\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning: invalid escape sequence '\\S'\n"
     ]
    }
   ],
   "source": [
    "escape_list=[]\n",
    "stop=[]\n",
    "#Cleaning the train set\n",
    "train['title']=train['title'].apply(text_cleaning,args=(escape_list,stop))\n",
    "train['text']=train['text'].apply(text_cleaning,args=(escape_list,stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['label']=='REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1976"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['label']=='FAKE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean=train[(train['label']=='REAL')|(train['label']=='FAKE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REAL    1990\n",
       "FAKE    1976\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_clean=pd.read_csv('train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_clean[['title','text']]\n",
    "y=pd.get_dummies(train_clean['label'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two ways you can do this. One is the easy one. Adding the title and text into one column and then using tfidf. Or use two column tfidf based on Pipeline. We will work with both and see their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abandyop\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding title and text to one column.\n",
    "X['title_text'] = X['title']+'  '+X['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abandyop\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification Report:- \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.94      0.91       418\n",
      "          1       0.93      0.88      0.90       376\n",
      "\n",
      "avg / total       0.91      0.91      0.91       794\n",
      "\n",
      "Confusion Matrix:- \n",
      "\n",
      "[[392  26]\n",
      " [ 47 329]]\n",
      "\n",
      "Accuracy : 0.9081\n",
      "AUC Score (Train): 0.906400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X_tfidf = vec.fit_transform(X['title_text']).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#THIS LINE IS WHERE WE ARE SPLITTING\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.20, random_state=13)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "from sklearn.metrics import classification_report\n",
    "predicted=logmodel.predict(X_test)\n",
    "print('The classification Report:- \\n')\n",
    "print(classification_report(y_test,predicted))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix:- \\n')\n",
    "print(confusion_matrix(y_test,predicted))\n",
    "from sklearn import metrics\n",
    "#Print model report:\n",
    "print(\"\\nAccuracy : %.4g\" % metrics.accuracy_score(y_test,predicted))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The hard way using two columns: Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('title_tfidf', Pipeline(memory=None,\n",
       "     steps=[('extract_field', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x000001F8EFC2CB70>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('tfi...      token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = FeatureUnion([\n",
    "                ('title_tfidf', \n",
    "                  Pipeline([('extract_field',\n",
    "                              FunctionTransformer(lambda x: x['title'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('text_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['text'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())]))]) \n",
    "\n",
    "transformer.fit(X[['title','text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_two=transformer.transform(X[['title','text']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3966, 48329)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_two.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3966, 42560)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3966, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abandyop\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification Report:- \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.94      0.92       418\n",
      "          1       0.94      0.89      0.91       376\n",
      "\n",
      "avg / total       0.92      0.92      0.92       794\n",
      "\n",
      "Confusion Matrix:- \n",
      "\n",
      "[[395  23]\n",
      " [ 42 334]]\n",
      "\n",
      "Accuracy : 0.9181\n",
      "AUC Score (Train): 0.916637\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression using Two Columns Chcek out the betterment in result\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_two, y, test_size=0.20, random_state=13)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "from sklearn.metrics import classification_report\n",
    "predicted=logmodel.predict(X_test)\n",
    "print('The classification Report:- \\n')\n",
    "print(classification_report(y_test,predicted))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix:- \\n')\n",
    "print(confusion_matrix(y_test,predicted))\n",
    "from sklearn import metrics\n",
    "#Print model report:\n",
    "print(\"\\nAccuracy : %.4g\" % metrics.accuracy_score(y_test,predicted))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abandyop\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification Report:- \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.94      0.92       418\n",
      "          1       0.94      0.89      0.91       376\n",
      "\n",
      "avg / total       0.92      0.92      0.92       794\n",
      "\n",
      "Confusion Matrix:- \n",
      "\n",
      "[[395  23]\n",
      " [ 42 334]]\n",
      "\n",
      "Accuracy : 0.9181\n",
      "AUC Score (Train): 0.916637\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with 'Linear' Kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_two, y, test_size=0.20, random_state=13)\n",
    "\n",
    "# Fitting SVM classifier to the Training set\n",
    "#SVM will take some time to train.DON'T WORRY\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear', random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "predicted = classifier.predict(X_test)\n",
    "print('The classification Report:- \\n')\n",
    "print(classification_report(y_test,predicted))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix:- \\n')\n",
    "print(confusion_matrix(y_test,predicted))\n",
    "from sklearn import metrics\n",
    "#Print model report:\n",
    "print(\"\\nAccuracy : %.4g\" % metrics.accuracy_score(y_test,predicted))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before testing on test data,let us train the model on the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the test set`\n",
    "test['title']=test['title'].apply(text_cleaning,args=(escape_list,stop))\n",
    "test['text']=test['text'].apply(text_cleaning,args=(escape_list,stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abandyop\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#For testing the Test File\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_tfidf_two, y)\n",
    "test_tfidf_two=transformer.transform(test[['title','text']]).toarray()\n",
    "test_predicted=logmodel.predict(test_tfidf_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placing the predictions in test dataset\n",
    "test['prediction']=test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the 1s as REAL and 0s as FAKE\n",
    "test['prediction']=test['prediction'].apply(lambda x: 'REAL' if x ==1 else 'FAKE' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the dataset as csv for submission\n",
    "test.loc[:,['ID','prediction']].to_csv('Submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
